{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "356594e0",
   "metadata": {},
   "source": [
    "# Principal Component Analysis Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f32f29",
   "metadata": {},
   "source": [
    "We begin PCA by importing the necessary libraries for PCA and one-hot encoding the categorical variables in our CCRB dataset. PCA is a mathematical technique that only operates on numerical data, so this is a crucial preprocessing step. Then, we needed to standardize our present data. This is another important step since PCA is sensitive to the scale of the variables. Without standardization, variables with larger variance will dominate the principal components, regardless of their true importance. Standardizing the data before running PCA ensures that all features are given equal importance, and the principal components are based on the actual variations in the data, rather than the scale of the features. Finally, we have to check for Nan and infinite values once the standardization is complete. `StandardScaler` performs a mathematical operation that scales the data by subtracting the mean of each feature and dividing by its standard deviation. If a feature has a standard deviation of zero, this will result in dividing by zero, which produces an infinite value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8f54827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries for PCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load dataset\n",
    "ccrb = pd.read_csv(\"CCRB Complaint Database Raw 04.20.2021.csv\")\n",
    "ccrb = ccrb.drop(columns = [\"AsOfDate\", \"AllegationID\", \"LastName\", \"FirstName\", \"ShieldNo\", \"ComplaintID\",\n",
    "                            \"PenaltyRec\", \"NYPDDisposition\", \"PenaltyDesc\", \"LocationType\", \"ReceivedDate\",\n",
    "                            \"CloseDate\", \"IncidentRank\", \"CurrentRank\", \"BoardCat\", \"OfficerID\", \"LastActive\"])\n",
    "\n",
    "# lowercase all column names and strings in column names\n",
    "ccrb.columns= ccrb.columns.str.lower()\n",
    "for column in list(ccrb.columns):\n",
    "    if type(ccrb[column][0]) == str:\n",
    "        ccrb[column] = ccrb[column].str.lower()\n",
    "    \n",
    "# binarize the CCRBDisposition column between (0) unsubstantiated and (1) substantiated\n",
    "# if substantiated, switch to substantiated; if anything else, then unsubstantiated\n",
    "ccrbDispositionList = list(ccrb[\"ccrbdisposition\"])\n",
    "for i in range(len(ccrbDispositionList)):\n",
    "    if \"substantiated\" in ccrbDispositionList[i] and \"unsubstantiated\" not in ccrbDispositionList[i]:\n",
    "        ccrbDispositionList[i] = 1\n",
    "    else:\n",
    "        ccrbDispositionList[i] = 0\n",
    "ccrb[\"ccrbdisposition\"] = ccrbDispositionList\n",
    "ccrb = ccrb.drop(columns = [\"incidentdate\"])\n",
    "ccrb = ccrb.drop(ccrb[ccrb[\"daysonforce\"] < 0].index)\n",
    "ccrb = ccrb.drop(ccrb[ccrb[\"impactedage\"] < 0].index)\n",
    "ccrb = ccrb.drop(ccrb[ccrb[\"impactedage\"] > 116].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d738dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[     6      1]\n",
      " [     7      1]\n",
      " [    38      1]\n",
      " ...\n",
      " [279401      1]\n",
      " [279402      1]\n",
      " [279403      1]]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# select the columns to one-hot encode\n",
    "categorical_cols = ['officerrace', 'officergender', 'currentranklong', 'currentcommand',\n",
    "                    'incidentranklong', 'incidentcommand', 'status', 'fadotype',\n",
    "                    'allegation', 'ccrbdisposition', 'contactreason', 'contactoutcome',\n",
    "                    'incidentprecinct', 'impactedrace', 'impactedgender']\n",
    "\n",
    "# create a new data frame with one-hot encoded categorical columns\n",
    "ccrb_onehot = pd.get_dummies(ccrb, columns=categorical_cols)\n",
    "\n",
    "# standardize data (important since PCA is sensitive to scale of variables)\n",
    "scaler = StandardScaler()\n",
    "ccrb_std = scaler.fit_transform(ccrb_onehot)\n",
    "\n",
    "# check for NaN and infinite values\n",
    "print(np.argwhere(np.isnan(ccrb_std)))\n",
    "print(np.argwhere(np.isinf(ccrb_std)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfbd15f",
   "metadata": {},
   "source": [
    "In order to PCA to run, we must drop all missing and infinite values from our standardized dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abc808f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with missing values after data standardization\n",
    "ccrb_std = ccrb_std[~np.isnan(ccrb_std).any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d163dc2",
   "metadata": {},
   "source": [
    "Before we run the PCA algorithm on our standardized dataset, we can see that the dataset currently as a shape of (156413, 1727). We can decrease this tremendously with PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0f17929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156413, 1727)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# standardized data before dimentionality reduction\n",
    "ccrb_std.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2216e7",
   "metadata": {},
   "source": [
    "Now, we can fit the PCA algorithm to our clean, standardized data and plot the cumulative expected variance ratio (CEVR) in order to determine the optimal number of principal components. This plot shows the proportion of the total variance in the data explained by each principal component. As shown below, the range of principal components from 1250 to 1500 is the optimal range we can use on our dataset without losing much of the original variance in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a10ca89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create instance of PCA class and fit it to standardized data\n",
    "pca = PCA()\n",
    "pca.fit(ccrb_std)\n",
    "\n",
    "# plot the explained variance ratio to determine optimal number of principal components\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_.cumsum(), marker='o')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d90a1c0",
   "metadata": {},
   "source": [
    "We chose to incorporate the lower end of the \"elbow point\" in our plot. While 1500 principal components would represent our original data without losing any information, using all the principal components may not always be the best option, as it can lead to overfitting and poor generalization to new data. It's recommended to select a smaller number of principal components that explain most of the variance in the data, while still keeping the overall number of dimensions low. Therefore, we chose 1250 principal components as our optimal number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c228cb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the optimal number of principal components based on the explained variance ratio\n",
    "n_components = 1250\n",
    "\n",
    "# create a new instance of PCA with the optimal number of components\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "# fit the PCA to the standardized data\n",
    "pca.fit(ccrb_std)\n",
    "\n",
    "# apply the PCA transformation to the original dataset\n",
    "ccrb_pca = pca.transform(ccrb_std)\n",
    "\n",
    "# convert the numpy array to a pandas dataframe\n",
    "ccrb_pca_df = pd.DataFrame(ccrb_pca, columns=['PC{}'.format(i) for i in range(1, n_components + 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f4cac4",
   "metadata": {},
   "source": [
    "Our new dataframe is now (156413, 1250), which is an excellent improvement from our original shape of (156413, 1727)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68baedff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ccrb_pca_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a95313",
   "metadata": {},
   "outputs": [],
   "source": [
    "ccrb_pca_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5534f659",
   "metadata": {},
   "source": [
    "When using PCA, the new column names generated by the transformation will correspond to the principal components themselves (i.e., \"PC1\", \"PC2\", etc.). These new columns represent linear combinations of the original features that capture the most variance in the data.\n",
    "\n",
    "In general, it's not possible to directly interpret the principal components in terms of the original features, since they are not just simple combinations of the original features. However, it is possible to examine the loadings of each feature on each principal component to gain some insight into which original features are most strongly associated with each principal component. This process is done below. Each row in `loadings_df` corresponds to a principal component and each column corresponds to an original feature, with the values in the cells representing the weight of each feature on each principal component. We can use this data frame to interpret the relationship between the original features and the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790cb096",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
